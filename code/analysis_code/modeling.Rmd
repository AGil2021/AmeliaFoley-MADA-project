---
title: "modeling"
output: html_document
---

In this portion of the analysis, we will use the LASSO method for regularization of the model. We'll also try out a decision tree

# load packages and data
```{r}
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) #for modeling
library(rpart)
library(glmnet)
library(ranger)
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots
library(gridExtra) # for savings images in grids

#path to data
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","processeddata.rds")

#load cleaned data. 
data <- readRDS(here("data", "processed_data", "mpwrfcombo.rds"))
```

# split data into train and test subsets
```{r}
# set seed for reproducible analysis (instead of random subset each time)
set.seed(123)
#subset 3/4 of data as training set
data_split <- initial_split(data, 
                            prop = 7/10, 
                            strata = particles_l) #stratify by MP concentration for balanced outcome

#save sets as data frames
train_data <- training(data_split)
test_data <- testing(data_split)
```

# Cross validation
We want to perform 5-fold CV, 5 times repeated
```{r}
#create folds (resample object)
set.seed(123)
folds <- vfold_cv(train_data, 
                  v = 5, 
                  repeats = 5,
                  strata = particles_l) #folds is set up to perform our CV

#linear model set up
lm_mod <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')

#create recipe for data and fitting and make dummy variables
MP_rec <- recipe(particles_l ~ ., data = train_data) %>% step_dummy(all_nominal())

#workflow set up
MP_wflow <- 
  workflow() %>% add_model(lm_mod) %>% add_recipe(MP_rec)

#use workflow to prepare recipe and train model with predictors
MP_fit <- 
  MP_wflow %>% fit(data = train_data)

#extract model coefficient
MP_fit %>% extract_fit_parsnip() %>% tidy()
```

# Null model performance
```{r}
#recipe for null model
null_train_rec <- recipe(particles_l ~ 1, data = train_data) #predicts mean of outcome

#null model workflow incorporating null model recipe
null_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_train_rec)

# I want to check and make sure that the null model worked as it was supposed to, so I want to view the predictions and make sure they are all the mean of the outcome
#get fit for train data using null workflow
nullfittest <- null_wflow %>% fit(data = train_data)
#get predictions based on null model
prediction <- predict(nullfittest, train_data)
test_pred <- predict(nullfittest, test_data)
#the predictions for the train and test data are all the same mean value, so this tells us the null model was set up properly

#Now, we'll use fit_resamples based on the tidymodels tutorial for CV/resampling (https://www.tidymodels.org/start/resampling/)
#fit model with training data
null_fit_train <- fit_resamples(null_wflow, resamples = folds)

#get results
metrics_null_train <- collect_metrics(null_fit_train)
#RMSE for null train fit is 109.4

#repeat for test data
null_test_rec <- recipe(particles_l ~ 1, data = test_data) #predicts mean of outcome
null_test_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_test_rec) #sets workflow with new test recipe
null_fit_test <- fit_resamples(null_test_wflow, resamples = folds) #performs fit
metrics_null_test <- collect_metrics(null_fit_test) #gets fit metrics
#RMSE for null test fit is 109.4
```

# Model tuning and fitting
Include:
1. Model specification
2. Workflow definition
3. Tuning grid specification
4. Tuning w/ cross-validation + `tune_grid()`

## LASSO model
```{r}
#based on tidymodels tutorial: case study
#cross validation
set.seed(123)
cell_folds <- vfold_cv(train_data)

#model specification
#lasso <- linear_reg(penalty = tune()) %>% set_engine("glmnet") %>% set_mode("regression")
lasso <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1)

#set workflow
lasso_wf <- workflow() %>% add_model(lasso) %>% add_recipe(MP_rec)

#tuning grid specification
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#tuning with CV and tune_grid
lasso_res <- lasso_wf %>% tune_grid(resamples = cell_folds, 
                                    grid = lasso_grid, 
                                    control = control_grid(save_pred = TRUE), 
                                    metrics = metric_set(rmse))
#view model metrics
lasso_res %>% collect_metrics()

#select top models
top_lasso <- 
  lasso_res %>% show_best("rmse") %>% arrange(penalty)
top_lasso #view

#see best lasso
best_lasso <- lasso_res %>% select_best()
best_lasso #view

#finalize workflow with top model
lasso_final_wf <- lasso_wf %>% finalize_workflow(best_lasso)

#fit model with finalized WF
lasso_fit <- lasso_final_wf %>% fit(train_data)
```

### LASSO plots
```{r}
#diagnostics
autoplot(lasso_res)
#calculate residuals
lasso_resid <- lasso_fit %>%
  augment(train_data) %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
lasso_pred_plot <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "LASSO", x = "Outcome (Particles/L)", y = "Prediction (Particles/L)")+ ylim(0, 1000) + xlim(0,1000)
lasso_pred_plot
ggsave(lasso_pred_plot, filename = here("results", "lasso_pred.png"))

#plot residuals vs predictions
lasso_resid_plot <- ggplot(lasso_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "LASSO", x = "Prediction (Particles/L)", y = "Residuals")
lasso_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
lasso_res %>% show_best(n=1) #view RMSE for best lasso model

```

next I want to see what predictors are used in the best lasso model. also need to compare to null model

RMSE for best LASSO model is 100.5, compared to 109.4 RMSE for the null model. Still need to check included predictors for the best LASSO model. 

# plot for how the number of predictors included in the LASSO model changes with the tuning parameter
```{r}
x <- lasso_fit$fit$fit$fit 
plot(x, "lambda")

summary(best_lasso)

coefficients <- lasso_fit %>% extract_fit_parsnip() %>% tidy()
lasso_fit %>% extract_fit_parsnip() %>% tidy()

```

By looking at the predictors used in this fit, we see there are a lot of uninformative predictors included, such as latitude, longitude, site ID, FIPS, etc. Let's look at a subset of predictors to build a model. 

Try modeling with data subset, including these variables: "particles_l", "visual_score", "turbidity.ntu", "temperature.c", "e.coli.cfu", "population", "dist". We'll do this in a separate Rmd document called `subset_modeling.Rmd`

# Decision tree
Next, we'll try out a decision tree model

## Decision tree
```{r}
#going based off of tidymodels tutorial: tune parameters
#since we already split our data into test and train sets, we'll continue to use those here. they are `train_data` and `test_data`

#model specification
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")
tune_spec

#tuning grid specification
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
tree_grid

#cross validation
set.seed(123)
cell_folds <- vfold_cv(train_data)

#workflow
set.seed(123)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(MP_rec)

#model tuning with `tune_grid()`
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
    )
tree_res %>% collect_metrics()
#Here we see 25 candidate models, and the RMSE and Rsq for each
tree_res %>% autoplot() #view plot

#select the best decision tree model
best_tree <- tree_res %>% select_best("rmse")
best_tree #view model details

#finalize model workflow with best model
tree_final_wf <- tree_wf %>%
  finalize_workflow(best_tree) 

#fit model
tree_fit <- 
  tree_final_wf %>% fit(train_data)
tree_fit

```


### Decision tree plots
```{r}
#diagnostics
autoplot(tree_res)
#calculate residuals - originally got stuck trying out lots of different methods for this. took inspiration from Zane's code to manually calculate residuals rather than using some of the built in functions that I could not get to cooperate
tree_resid <- tree_fit %>%
  augment(train_data) %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
tree_pred_plot <- ggplot(tree_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Decision Tree", x = "Outcome (Particles/L)", y = "Prediction (Particles/L)")+ ylim(0, 1000) + xlim(0,1000)
tree_pred_plot

#plot residuals vs predictions
tree_resid_plot <- ggplot(tree_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "Decision Tree", x = "Prediction (Particles/L)", y = "Residuals")
tree_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
tree_res %>% show_best(n=1) #view RMSE for best decision tree model
```

The best decision tree has an RMSE of 102.2, which is better than the null model RMSE of 109. However, the decision tree did not perform as well well as the Lasso regularized model, which had an RMSE of about 100. Based on the outcome/prediction plot, it appears that the model is making one of two predictions for any given actual outcome. This indicates that this is not a good model, as we don't see the expected 45 degree diagonal line where predictions correlate closely with outcomes. 

# Random forest
Finally, let's try out a random forest. 

### split data into train and test subsets with only COMPLETE CASES
```{r}
data %>% is.na() %>% summary() #check out missing cases
complete <- data %>% na.omit() #save complete cases
complete <- complete %>% select(-"visual_score")

# set seed for reproducible analysis (instead of random subset each time)
set.seed(123)
#subset 3/4 of data as training set
complete_data_split <- initial_split(complete, 
                            prop = 7/10, 
                            strata = particles_l) #stratify by MP concentration for balanced outcome

#save sets as data frames
complete_train_data <- training(complete_data_split)
complete_test_data <- testing(complete_data_split)

#reset recipe with adjusted, complete data
MP_rec <- recipe(particles_l ~ ., data = complete_train_data) %>% step_dummy(all_nominal())
#reset cell_folds
set.seed(123)
cell_folds <- vfold_cv(complete_train_data)
```

## Random forest
```{r}
#based on tidymodels tutorial: case study
library(parallel)
cores <- parallel::detectCores()
cores
#model specification
r_forest <- rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>% set_engine("ranger", num.threads = cores) %>% set_mode("regression")

#set workflow
r_forest_wf <- workflow() %>% add_model(r_forest) %>% add_recipe(MP_rec)

#tuning grid specification
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
#what we will tune:
r_forest %>% parameters()

#tuning with CV and tune_grid

r_forest_res <- 
  r_forest_wf %>%
  tune_grid(resamples = cell_folds, 
            grid = rf_grid, 
            control = control_grid(save_pred = TRUE), 
            metrics = metric_set(rmse))

#view top models
r_forest_res %>% show_best(metric = "rmse")

#view plot of models performance
autoplot(r_forest_res)

#select best model
rf_best <- r_forest_res %>% select_best(metric = "rmse")
rf_best

#finalize workflow with top model
rf_final_wf <- r_forest_wf %>% finalize_workflow(rf_best)

#fit model with finalized WF
rf_fit <- rf_final_wf %>% fit(complete_train_data)
```
Here, we get an error message that all models failed, note: missing value in visual_score. Let's go back and use only complete cases for the random forest. 

The issue was in "cell_folds" which had originated earlier in the Rmd and didn't include the updated complete case dataset. Added a line of code to reset cell_folds at this stage of the Rmd in order to use complete cases only. We still get some error messages but are able to run the model. 

The best random forest models have RMSEs of about 108. This is slightly better than the null model, but not significantly. The random forest model does not perform as well as the decision tree or LASSO model. 

### Random forest plots
```{r}
#diagnostics
autoplot(r_forest_res)
#calculate residuals
rf_resid <- rf_fit %>%
  augment(complete_train_data) %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
rf_pred_plot <- ggplot(rf_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Random Forest", x = "Outcome (Particles/L)", y = "Prediction (Particles/L)")+ ylim(0, 1000) + xlim(0,1000)
rf_pred_plot


#plot residuals vs predictions
rf_resid_plot <- ggplot(rf_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "Random Forest", x = "Prediction (Particles/L)", y = "Residuals")
rf_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
r_forest_res %>% show_best(n=1) #view RMSE for best decision tree model
```


# Model comparison plots
```{r}
model_grid <- grid.arrange(lasso_pred_plot, rf_pred_plot, tree_pred_plot, lasso_resid_plot, rf_resid_plot, tree_resid_plot, ncol = 3)

ggsave(model_grid, filename = here("results", "model_grid.png"))
```


# Model selection : LASSO
```{r}
#Based on model performance plots and RMSE, we've selected the LASSO model. Let's evaluate it with the test data. 

#fit to test data
last_lasso_fit <- lasso_final_wf %>% last_fit(data_split)
last_lasso_fit %>% collect_metrics()
```

It appears that the lasso model receives an RMSE of ~300 when run with the test data. Compared to the RMSE of 100 with the train data, this is not great performance and indicates that the model overfits the train data. 

# Variable importance
```{r}
library(parsnip)
var_importance <- last_lasso_fit %>% 
  purrr::pluck(".workflow", 1) %>%   
  workflows::extract_fit_parsnip() %>% 
  vip(num_features = 20)

ggsave(var_importance, filename = here("results", "var_importance.png"))
```

# Final lasso plots
```{r}
#diagnostics
autoplot(lasso_res)
#calculate residuals
last_lasso_resid <- last_lasso_fit %>%
  augment() %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
last_lasso_pred_plot <- ggplot(last_lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "LASSO: Pred vs Outcomes", x = "Test Outcome (Particles/L)", y = "Test Prediction (Particles/L)")+ ylim(0, 250) + xlim(0,250)
last_lasso_pred_plot
ggsave(last_lasso_pred_plot, filename = here("results", "last_lasso_pred.png"))

#plot residuals vs predictions
last_lasso_resid_plot <- ggplot(last_lasso_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "LASSO: Test Pred vs Residuals", x = "Test Prediction (Particles/L)", y = "Residuals")
last_lasso_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
last_lasso_fit %>% collect_metrics() #view RMSE for best lasso model

```

