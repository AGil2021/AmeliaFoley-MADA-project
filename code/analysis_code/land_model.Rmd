---
title: "land_model"
output: html_document
---


In this portion of the analysis, we will include the land use data for the sample sites to see if the additional predictors can improve our model fit. 

# load packages and data
```{r}
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) #for modeling
library(rpart)
library(glmnet)
library(ranger)
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots



#load cleaned land data. 
data <- readRDS(here("data", "processed_data", "athensland.rds"))
data <- data %>% select(-"site", 
                        -"lat",
                        -"long",
                        -"date")
```

# split data into train and test subsets
```{r}
# set seed for reproducible analysis (instead of random subset each time)
set.seed(123)
#subset 3/4 of data as training set
data_split <- initial_split(data, 
                            prop = 7/10, 
                            strata = particles_l) #stratify by MP concentration for balanced outcome

#save sets as data frames
train_data <- training(data_split)
test_data <- testing(data_split)
```

# Cross validation
We want to perform 5-fold CV, 5 times repeated
```{r}
#create folds (resample object)
set.seed(123)
folds <- vfold_cv(train_data, 
                  v = 5, 
                  repeats = 5,
                  strata = particles_l) #folds is set up to perform our CV

#linear model set up
lm_mod <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')

#create recipe for data and fitting and make dummy variables
MP_rec <- recipe(particles_l ~ ., data = train_data) %>% step_dummy(all_nominal())

#workflow set up
MP_wflow <- 
  workflow() %>% add_model(lm_mod) %>% add_recipe(MP_rec)

#use workflow to prepare recipe and train model with predictors
MP_fit <- 
  MP_wflow %>% fit(data = train_data)

#extract model coefficient
MP_fit %>% extract_fit_parsnip() %>% tidy()
```

# Null model performance
```{r}
#recipe for null model
null_train_rec <- recipe(particles_l ~ 1, data = train_data) #predicts mean of outcome

#null model workflow incorporating null model recipe
null_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_train_rec)

# I want to check and make sure that the null model worked as it was supposed to, so I want to view the predictions and make sure they are all the mean of the outcome
#get fit for train data using null workflow
nullfittest <- null_wflow %>% fit(data = train_data)
#get predictions based on null model
prediction <- predict(nullfittest, train_data)
test_pred <- predict(nullfittest, test_data)
#the predictions for the train and test data are all the same mean value, so this tells us the null model was set up properly

#Now, we'll use fit_resamples based on the tidymodels tutorial for CV/resampling (https://www.tidymodels.org/start/resampling/)
#fit model with training data
null_fit_train <- fit_resamples(null_wflow, resamples = folds)

#get results
metrics_null_train <- collect_metrics(null_fit_train)
#RMSE for null train fit is 109.4

#repeat for test data
null_test_rec <- recipe(particles_l ~ 1, data = test_data) #predicts mean of outcome
null_test_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_test_rec) #sets workflow with new test recipe
null_fit_test <- fit_resamples(null_test_wflow, resamples = folds) #performs fit
metrics_null_test <- collect_metrics(null_fit_test) #gets fit metrics
#RMSE for null test fit is 109.4
```

# Model tuning and fitting
Include:
1. Model specification
2. Workflow definition
3. Tuning grid specification
4. Tuning w/ cross-validation + `tune_grid()`

## LASSO model
```{r}

#cross validation
set.seed(123)
cell_folds <- vfold_cv(train_data)

#model specification
#lasso <- linear_reg(penalty = tune()) %>% set_engine("glmnet") %>% set_mode("regression")
lasso <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1)

#set workflow
lasso_wf <- workflow() %>% add_model(lasso) %>% add_recipe(MP_rec)

#tuning grid specification
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#tuning with CV and tune_grid
lasso_res <- lasso_wf %>% tune_grid(resamples = cell_folds, 
                                    grid = lasso_grid, 
                                    control = control_grid(save_pred = TRUE), 
                                    metrics = metric_set(rmse))
#view model metrics
lasso_res %>% collect_metrics()

#select top models
top_lasso <- 
  lasso_res %>% show_best("rmse") %>% arrange(penalty)
top_lasso #view

#see best lasso
best_lasso <- lasso_res %>% select_best()
best_lasso #view

#finalize workflow with top model
lasso_final_wf <- lasso_wf %>% finalize_workflow(best_lasso)

#fit model with finalized WF
lasso_fit <- lasso_final_wf %>% fit(train_data)
```

### LASSO plots
```{r}
#diagnostics
autoplot(lasso_res)
#calculate residuals
lasso_resid <- lasso_fit %>%
  augment(train_data) %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
lasso_pred_plot <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Particles/L Outcome", y = "Particles/L Prediction")
lasso_pred_plot


#plot residuals vs predictions
lasso_resid_plot <- ggplot(lasso_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", x = "Particles/L Prediction", y = "Residuals")
lasso_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
lasso_res %>% show_best(n=1) #view RMSE for best lasso model

```



RMSE for best LASSO model on the land data is 127.5, compared to 109.4 RMSE for the null model. This tells us our model is not good. 

In the perfect outcome/prediction plot, you hope to see data scattered along a 45 degree diagonal line. Instead, data is clustered. However, the scales of the x and y axis are different which may impact how we are viewing the data. Let's visualize with a better scale.

```{r}
#model predictions from tuned model vs actual outcomes SCALE ADJUSTED
lasso_pred_plot_adj <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Particles/L Outcome", y = "Particles/L Prediction") + ylim(0, 1000) + xlim(0,1000)
lasso_pred_plot_adj
```

Our outcome vs prediction plot isn't terrible but also isn't great. It looks like predictions are consistently higher than the actual outcomes, since the points fall below the hypothetical 45 degree diagonal line that would represent a perfect fit. 

#plot for how the number of predictors included in the LASSO model changes with the tuning parameter
```{r}
x <- lasso_fit$fit$fit$fit 
plot(x, "lambda")

summary(best_lasso)

coefficients <- lasso_fit %>% extract_fit_parsnip() %>% tidy()
lasso_fit %>% extract_fit_parsnip() %>% tidy()

```


It appears that the addition of land use data does not make our model more informative. In fact, it increases the RMSE to be higher than that of the null model. 








```
