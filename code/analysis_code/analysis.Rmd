---
title: "analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#load packages and data
```{r}
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) #for modeling

data <- readRDS(here("data", "processed_data", "mpwrfcombo.rds"))
```

# fit linear model to continuous outcome (particles_l)
```{r}
#attempt to visualize data
#look at particles per L vs population
ggplot(data, aes(x = population, y = particles_l)) + geom_point() + geom_smooth(method = lm)
```



# linear regression
```{r}
#linear regression model specification set up
lm_mod <- linear_reg() %>% set_engine("lm")
#estimating/training linear model
lm_fit1 <- lm_mod %>% fit(particles_l ~ ., data = data)
#view summary of fit
tidy(lm_fit1)
```

# linear regression for one predictor, one outcome
```{r}
#predict particles per liter based on population
lm_fit2 <- lm_mod %>% fit(particles_l ~ population, data = data)
#view summary of fit
tidy(lm_fit2)

#predict particles per liter based on e coli
lm_fit3 <- lm_mod %>% fit(particles_l ~ e.coli.cfu, data = data)
#view summary of fit
tidy(lm_fit3)

#comparison of fit
comp2_3 <- anova(lm_fit2$fit, lm_fit3$fit, test = "Chisq")
comp2_3
```

Here, we get an error message that models were not fitted to the same size of dataset. This is likely due to missing values in some predictors. To solve this, we can clean up the dataset some more and create a dataset specifically to use for modelling, that has only predictors of interest and no missing values. 

#clean up data for modelling
```{r}
data %>% is.na() %>% summary()
complete <- data %>% na.omit()
```

Trying code from above again with the complete dataset

```{r}
#estimating/training linear model
lm_fit4 <- lm_mod %>% fit(particles_l ~ ., data = complete)
#view summary of fit
tidy(lm_fit4)
#predict particles per liter based on population
lm_fit5 <- lm_mod %>% fit(particles_l ~ population, data = complete)
#view summary of fit
tidy(lm_fit5)

#predict particles per liter based on e coli
lm_fit6 <- lm_mod %>% fit(particles_l ~ e.coli.cfu, data = complete)
#view summary of fit
tidy(lm_fit6)

#comparison of fit
comp5_6 <- anova(lm_fit5$fit, lm_fit6$fit, test = "Chisq")
comp5_6
```

Here, we have a VERY large RSS, which I think indicates a poor model fit for our data. Let's try with some variables removed. 

# Subsetting data to remove unimportant variables
```{r}
subset <- complete %>% select(-"site", 
                              "lat", 
                              "long",
                              "date") 
#estimating/training linear model
lm_fit7 <- lm_mod %>% fit(particles_l ~ ., data = subset)
#view summary of fit
tidy(lm_fit7)    

#predict particles per liter based on population
lm_fit8 <- lm_mod %>% fit(particles_l ~ population, data = subset)
#view summary of fit
tidy(lm_fit8)

#predict particles per liter based on e coli
lm_fit9 <- lm_mod %>% fit(particles_l ~ e.coli.cfu, data = subset)
#view summary of fit
tidy(lm_fit9)

#model comparison
comp8_9 <- anova(lm_fit8$fit, lm_fit9$fit, test = "Chisq")
comp8_9
```

Here, we see the same RSS results even after subsetting the data (the values are exactly the same when run on the "complete" data as on the "subset" data.)

# model evaluation, splitting data into train and test
```{r}
#split into test and train
#set seed for reproducible analysis
set.seed(222)
#subset 3/4 of data as training set
data_split <- initial_split(data, prop = 3/4)

#save sets as data frames
train_data <- training(data_split)
test_data <- testing(data_split)
```

# recipe to fit continuous outcome to all predictors
```{r}
#recipe for particles per liter, all predictors
mp_rec <- 
  recipe(particles_l ~ ., data = train_data)

#linear regression model workflow set up
mp_wflow <- 
  workflow() %>% add_model(lm_mod) %>% add_recipe(mp_rec)

#use workflow to prepare recipe and train model with predictors
mp_fit <- 
  mp_wflow %>% fit(data = train_data)

#extract model coefficient
mp_fit %>% extract_fit_parsnip() %>% tidy()
```

Here, we see site names come up first, even though we know site names are not going to be good predictors of microplastic levels. I need to go back and remove the variables that aren't going to be helpful predictors (ex: site name) 

# evaluate model with ROC and ROC-AUC
```{r}
#use trained workflow to predict with test data
predict(mp_fit, test_data)

#include probabilities
mp_aug <- augment(mp_fit, test_data)

#get RMSE for test data
mp_aug %>% rmse(truth = particles_l, .pred)

```

Why are there so many rows resulting? When we did the analysis last week, only one row was the result, and it was just rmse and provided a value around 1. 



#trying model evaluation with SUBSET
```{r}

```

